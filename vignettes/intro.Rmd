---
title: "Piggyback Data atop your GitHub Repository!"
author: "Carl Boettiger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Piggyback Data atop your GitHub Repository!}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  results="hide",
  eval = Sys.getenv("CBOETTIG_TOKEN", FALSE)
)

Sys.setenv(piggyback_cache_duration="1e-6")

```


# Why `piggyback`?

`piggyback` grew out of the needs of students both in my classroom and in my research group, who frequently need to work with data files somewhat larger than one can conveniently manage by committing directly to GitHub.  As we frequently want to share and run code that depends on >50MB data files on each of our own machines, on continuous integration (i.e. [travis](https://travis.org)), and on larger computational servers, data sharing quickly becomes a bottleneck. 

## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("cboettig/piggyback")
```

## Authentication

No authentication is required to download data from *public* GitHub repositories using `piggyback`. Nevertheless, `piggyback` recommends setting a token when possible to avoid rate limits. To upload data to any repository, or to download data from *private* repositories, you will need to authenticate first. 

To do so, add your [GitHub Token](https://github.com/settings/tokens/new?scopes=repo,gist&description=R:GITHUB_PAT) to an environmental variable, e.g. in a `.Renviron` file in your home directory or project directory (any private place you won't upload), see `usethis::edit_r_environ()`.  For one-off use you can also sey your token from the R console using:

```r
Sys.setenv(GITHUB_TOKEN="xxxxxx")
```

But try to avoid putting `Sys.setenv()` in  any R scripts -- remember, the goal here is to avoid writing your private token in any file that might be shared, even privately.   For more help setting up a GitHub token, for the first time, see `usethis::browse_github_pat()`.


## Downloading data

Download the latest version or a specific version of the data:

```{r}
library(piggyback)
pb_download( "data/mtcars.tsv.gz", repo = "cboettig/piggyback")
```

Or a specific version:

```{r}
pb_download("mtcars.tsv.gz", repo = "cboettig/piggyback", tag = "v0.0.4")
```

Or simply omit the file name to download all assets connected with a given release.  (You can also always specify a destination directory to download. This directory will be created if it does not yet exist.)

```{r results="hide"}
pb_download(repo = "cboettig/piggyback")
```  


Sometimes it is preferable to have a URL from which the data can be read in directly, rather than downloading the data to a local file.  For example, such a URL can be embedded directly into another R script, avoiding any dependence on `piggyback` (provided the repository is already public.)  To get a list of URLs rather than actually downloading the files, use `pb_download_url()`:

```{r  results="hide"}
pb_download_url("mtcars.tsv.gz") 
```

Or store and read the URL directly into R:

```{r}
url <- pb_download_url("mtcars.tsv.gz") 
readr::read_tsv(url)
```


## Uploading data

If your GitHub repository doesn't have any [releases](https://help.github.com/articles/creating-releases/) yet, `piggyback` will help you quickly create one.  Create new releases to manage multiple versions of a given data file. While you can create releases as often as you like, making a new release is by no means necessary each time you upload a file.  If maintaining old versions of the data is not useful, you can stick with a single release and upload all of your data there.  

```{r eval=FALSE}
pb_new_release("cboettig/piggyback", "v0.0.1")
```

Once we have at least one release available, we are ready to upload.  By default, `pb_upload` will attach data to the latest release.  

```{r}
## We'll need some example data first.
## Pro tip: compress your tabular data to save space & speed upload/downloads
readr::write_tsv(mtcars, "mtcars.tsv.gz")

pb_upload("mtcars.tsv.gz", repo = "cboettig/piggyback")
```

You can also simply overwrite the a previous version of the file on an existing release, rather than creating a new release every time:

```{r  results="hide"}
pb_upload("mtcars.tsv.gz", repo = "cboettig/piggyback", overwrite = TRUE)
```

This is useful in scripts that may automatically upload their results, or whenever a previous version of a data file is disposable.  

## Additional convenience functions

List all files currently piggybacking on a given release (as always, defaults to `latest` if no `tag` is given):


```{r}
pb_list(repo = "cboettig/piggyback", tag = "data")
```

Delete a file from a release:

```{r eval = FALSE}
pb_delete(file = "mtcars.tsv.gz", repo = "cboettig/piggyback")
```

Note that this is irreversible unless you have a copy of the data elsewhere.  You can also simply set `overwrite=TRUE` in `pb_upload()` to overwrite an existing file.  

## git-style tracking

For an even simpler way to sync many data files to GitHub, `piggyback` provides Git-LFS-like `push` and `pull` methods.  These methods are simple wrappers around the basic upload and download interface that streamline the process of managing a potentially large number of data files of a given type or location.  

By default, specify a `glob` pattern of files to track.  `pb_track` can also track all files at a given path.  These patterns are simply written into a hidden config file, `.pbattributes` (just like `.gitattributes` in Git LFS), which you can also edit manually.  

```{r}
pb_track(c("*.tsv.gz", "*.tif", "*.zip"))
pb_track("data/*")
```

Adding a pattern with `pb_track()` will also automatically add that pattern to `.gitignore`, since these data files will be piggybacking on top of the repo rather than being version managed by `git`.  You probably will want to check in the `.pbattributes` file to version control, just as you would a `.gitattributes` or `.gitignore`. 


Upload all tracked data to GitHub.  By default, `pb_track()` will return a list of all tracked files which can be easily passed (or piped) to `pb_upload`.

```{r}
library(magrittr)
pb_track() %>% pb_upload()
```

Similarly, you can download all current data assets of the latest or specificed release by using pb_download without additional arguments: 

```{r}
pb_download()
```


## Timestamps and caching

By default, files will not be transfered if the timestamp is more recent at the destination (e.g. `pb_download()` will not download a copy if the local version is newer than the uploaded version).  If necessary, you can override this behavior by setting `use_timestamps = FALSE` in `pb_download()` or `pb_upload()`.  

To reduce API calls to GitHub, piggyback caches most calls with a timeout of 1 second by default.  This avoids repeating identical requests to update it's internal record of the repository data (releases, assets, timestamps, etc) during programmatic use.  You can increase or decrease this delay by setting the environmental variable in seconds, e.g. `Sys.setenv("piggyback_cache_duration"=10)` for a longer delay.  Note that the delay cannot be less than `1e-6` seconds at this time.


## Path names

GitHb assets attached to a release do not support file paths, and will convert most special characters (`#`, `%`, etc) to `.` or throw an error (e.g. for filenames containing `$`, `@`, `/`).  To preserve path information on uploading data, `piggyback` uses relative paths (relative to the working directory, or for `pb_push()` and `pb_pull`, relative to the project directory, see `here::here()`) in data file names, and encodes the system path delimiter as `.2f` (`%2f` is the HTML encoding of a literal `/`, but `%` cannot be used in asset names).  `piggyback` functions will always show and use the decoded file names, e.g. `data/mtcars.csv`, but you'll see `data.2fmtcars.csv` if you look at the release attachment on GitHub.


## A Note on GitHub Releases vs Data Archiving

`piggyback` is not intended as a data archiving solution.  Importantly, bare in mind that there is nothing special about multiple "versions" in releases, as far as data assets uploaded by `piggyback` are concerned.  The data files `piggyback` attaches to a Release can be deleted or modified at any time -- creating a new release to store data assets is the functional equivalent of just creating new directories `v0.1`, `v0.2` to store your data.  (GitHub Releases are always pinned to a particular `git` tag, so the code/git-managed contents associated with repo are more immutable, but remember our data assets just piggyback on top of the repo).  

Permanent, published data should always be archived in a proper data repository with a DOI, such as [zenodo.org](https://zenodo.org). Zenodo can freely archive public research data files up to 50 GB in size, and data is strictly versioned (once released, a DOI always refers to the same version of the data, new releases are given new DOIs). `piggyback` is meant only to lower the friction of working with data during the research process.  (e.g. provide data accessible to collaborators or continuus integration systems during research process, including for private repositories.)










